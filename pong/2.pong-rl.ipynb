{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-20 15:59:14,289] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find old network weights\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "# Configuration\n",
    "SUMMARY_DIR = './logs/pong'\n",
    "CHECKPOINT_DIR = './checkpoints/pong'\n",
    "\n",
    "gamma = 0.9  # discount factor for reward\n",
    "resume = True  # resume from previous checkpoint?\n",
    "render = False\n",
    "\n",
    "stack_size = 5\n",
    "\n",
    "# Optimizer configuration\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-2, decay=.99)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "\n",
    "# We will exploit easiest game, pong\n",
    "env = gym.make('Pong-v0')\n",
    "env.reset()\n",
    "\n",
    "action_space = [0, 2, 3]  # No-op / Up / Down\n",
    "n_action_space = len(action_space)\n",
    "\n",
    "\n",
    "#### Helper functions\n",
    "def preprocess(img):\n",
    "    from skimage.color import rgb2gray\n",
    "    from skimage.filters import threshold_mean\n",
    "    from skimage.measure import block_reduce\n",
    "    img = rgb2gray(img)\n",
    "\n",
    "    # from skimage.filters import try_all_threshold\n",
    "    # try_all_threshold(img, figsize=(10, 8), verbose=False)\n",
    "    # plt.show()\n",
    "\n",
    "    binary = img > threshold_mean(img)\n",
    "    binarized_img = binary.astype(int)\n",
    "    cropped_img = binarized_img[32:195]\n",
    "    reduced_img = block_reduce(cropped_img, block_size=(2, 2), func=np.max)\n",
    "\n",
    "    final_img = reduced_img * 255\n",
    "    return final_img * 255\n",
    "\n",
    "\n",
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward\n",
    "        http://karpathy.github.io/2016/05/31/rl/  \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        if r[t] != 1:\n",
    "            # reset the sum, since this was a game boundary (pong specific!)\n",
    "            running_add = 0\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "\n",
    "    # compute the discounted reward backwards through time\n",
    "    # standardize the rewards to be unit normal (helps control the gradient\n",
    "    # estimator variance)\n",
    "    discounted_r -= np.mean(discounted_r)\n",
    "    discounted_r /= np.std(discounted_r)\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "# get sample input\n",
    "observation, _, _, _ = env.step(random.choice(action_space))\n",
    "x = preprocess(observation)\n",
    "height, width = x.shape\n",
    "\n",
    "# based on input size, make model\n",
    "# net = PongNetwork(height=height, width=width, name='pongnet')\n",
    "\n",
    "with tf.name_scope(\"cnn\"):\n",
    "    X = tf.placeholder(tf.float32, [None, height, width, stack_size], name=\"input_x\")\n",
    "    x_stacked_image = tf.reshape(X, [-1, height, width, stack_size])\n",
    "\n",
    "    # Build a convolutional layer random initialization\n",
    "    W_conv1 = tf.get_variable(\"W_conv1\", shape=[5, 5, stack_size, 32], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    # W is [row, col, channel, feature]\n",
    "    b_conv1 = tf.Variable(tf.zeros([32]), name=\"b_conv1\")\n",
    "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x_stacked_image, W_conv1, strides=[1, 2, 2, 1], padding='VALID') + b_conv1,\n",
    "                         name=\"h_conv1\")\n",
    "\n",
    "    W_conv2 = tf.get_variable(\"W_conv2\", shape=[5, 5, 32, 64], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_conv2 = tf.Variable(tf.zeros([64]), name=\"b_conv2\")\n",
    "    h_conv2 = tf.nn.relu(tf.nn.conv2d(h_conv1, W_conv2, strides=[1, 2, 2, 1], padding='VALID') + b_conv2,\n",
    "                         name=\"h_conv2\")\n",
    "\n",
    "    W_conv3 = tf.get_variable(\"W_conv3\", shape=[5, 5, 64, 64], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_conv3 = tf.Variable(tf.zeros([64]), name=\"b_conv3\")\n",
    "    h_conv3 = tf.nn.relu(tf.nn.conv2d(h_conv2, W_conv3, strides=[1, 2, 2, 1], padding='VALID') + b_conv3,\n",
    "                         name=\"h_conv3\")\n",
    "\n",
    "    # Build a fully connected layer with softmax\n",
    "    h_conv3_flat = tf.reshape(h_conv3, [-1, 7 * 7 * 64], name=\"h_pool2_flat\")\n",
    "    W_fc1 = tf.get_variable(\"W_fc1\", shape=[7 * 7 * 64, n_action_space],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_fc1 = tf.Variable(tf.zeros([n_action_space]), name='b_fc1')\n",
    "    action_pred = tf.nn.softmax(tf.matmul(h_conv3_flat, W_fc1) + b_fc1, name=\"h_fc1\")\n",
    "\n",
    "    tf.summary.histogram(\"action_pred\", action_pred)\n",
    "\n",
    "    # Why do I use softmax here?\n",
    "    # http://home.deib.polimi.it/restelli/MyWebSite/pdf/rl7.pdf\n",
    "\n",
    "\n",
    "# We need to define the parts of the network needed for learning a policy\n",
    "Y = tf.placeholder(tf.float32, [None, n_action_space], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32, [None, 1], name=\"reward_signal\")\n",
    "\n",
    "# Loss function\n",
    "# Sum (Ai*logp(yi|xi))\n",
    "log_lik = -Y * (tf.log(tf.clip_by_value(action_pred, 1e-7, 1.0)))\n",
    "loss = tf.reduce_mean(tf.reduce_sum(log_lik * advantages, axis=1))\n",
    "tf.summary.scalar(\"A_pred\", tf.reduce_mean(action_pred))\n",
    "tf.summary.scalar(\"Y\", tf.reduce_mean(Y))\n",
    "tf.summary.scalar(\"log_likelihood\", tf.reduce_mean(log_lik))\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "# Learning\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# Some place holders for summary\n",
    "summary_reward = tf.placeholder(tf.float32, shape=(), name=\"reward\")\n",
    "tf.summary.scalar(\"reward\", summary_reward)\n",
    "\n",
    "# Summary\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "# Setting up our environment\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "writer = tf.summary.FileWriter(SUMMARY_DIR)\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "# Savor and Restore\n",
    "saver = tf.train.Saver()\n",
    "checkpoint = tf.train.get_checkpoint_state(CHECKPOINT_DIR)\n",
    "if checkpoint:\n",
    "    try:\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Successfully loaded:\", save_path)\n",
    "    except:\n",
    "        print(\"Error on loading old network weights\")\n",
    "else:\n",
    "    print(\"Could not find old network weights\")\n",
    "\n",
    "\n",
    "# run\n",
    "def perform_train():\n",
    "    save_path = \"model.ckpt\"\n",
    "    rendered_img = None\n",
    "    global_step = 0\n",
    "    for episode in range(1, 9999):\n",
    "        global_step += 1\n",
    "\n",
    "        xs_list = []\n",
    "        ys_list = []\n",
    "        rewards = np.empty(0).reshape(0, 1)\n",
    "        ep_rewards_list = []\n",
    "\n",
    "        state = env.reset()\n",
    "        state = preprocess(state)\n",
    "\n",
    "        stacked_state = [state] * stack_size  # initial input\n",
    "\n",
    "        ep_win_count = 0\n",
    "        ep_lose_count = 0\n",
    "        ep_reward_sum = 0\n",
    "\n",
    "        while True:\n",
    "            x = np.concatenate([stacked_state])  # list of (h, w) -> numpy (stack_size, h, w)\n",
    "            x = np.rollaxis(x, 0, 3)  # (stack_size, h, w) -> (h, w, stack_size)\n",
    "            x = x[None, :, :, :]  # (h, w, stack_size) -> (1, h, w, stack_size)\n",
    "            # Run the neural net to determine output\n",
    "            action_prob = sess.run(action_pred, feed_dict={X: x})\n",
    "            action_prob = np.squeeze(action_prob)  # shape (?, n) -> n\n",
    "            \n",
    "            # random_noise = np.random.uniform(0, 0.01, n_action_space)  # Add randomness!\n",
    "            # action_prob = (action_prob + random_noise) / np.sum(action_prob + random_noise)\n",
    "            action_index = np.random.choice(range(n_action_space), size=1, p=action_prob)[0]\n",
    "\n",
    "            action = action_space[action_index]\n",
    "\n",
    "            # random_noise = np.random.uniform(0, 1, n_action_space)\n",
    "            # action_index = np.random.choice(range(n_action_space), size=1, p=action_prob + random_noise)\n",
    "            # action = np.argmax(action_prob + random_noise)\n",
    "            # print(\"Action prediction: \", np.argmax(action_prob), \" action taken:\", action,\n",
    "            #      np.argmax(action_prob) == action)\n",
    "\n",
    "            # Append the observations and outputs for learning\n",
    "            xs_list.append(x)\n",
    "            y = np.eye(n_action_space)[action_index:action_index + 1]  # One hot encoding\n",
    "            ys_list.append(y)\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # env.render()\n",
    "            state = preprocess(state)\n",
    "            stacked_state = stacked_state[1:] + [state]  # FIFO\n",
    "            ep_reward_sum += reward\n",
    "\n",
    "            ep_rewards_list.append(reward)\n",
    "\n",
    "            # Discount rewards on every single game\n",
    "            if reward == 1 or reward == -1:\n",
    "                ep_rewards = np.vstack(ep_rewards_list)\n",
    "                discounted_rewards = discount_rewards(ep_rewards, gamma)\n",
    "                rewards = np.vstack([rewards, discounted_rewards])\n",
    "                ep_rewards_list = []\n",
    "                # print(\"Ep reward {}\".format(reward))\n",
    "\n",
    "                if reward == 1:\n",
    "                    ep_win_count += 1\n",
    "                else:\n",
    "                    ep_lose_count += 1\n",
    "\n",
    "            if done:\n",
    "                xs = np.vstack(xs_list)\n",
    "                ys = np.vstack(ys_list)\n",
    "\n",
    "                l, s, _ = sess.run([loss, summary, train],\n",
    "                                   feed_dict={X: xs,\n",
    "                                              Y: ys,\n",
    "                                              advantages: rewards,\n",
    "                                              summary_reward: ep_reward_sum})\n",
    "                writer.add_summary(s, global_step)\n",
    "                print(\"Episode {} Win {} Lose {} Reward {} Loss {}\".format(\n",
    "                    episode, ep_win_count, ep_lose_count, ep_reward_sum, l))\n",
    "                break\n",
    "\n",
    "            if render:\n",
    "                title = 'episode{}'.format(episode)\n",
    "                plt.title(title)\n",
    "\n",
    "                # you can directly render x (which is preprocessed game image)\n",
    "                if rendered_img is None:\n",
    "                    rendered_img = plt.imshow(state)\n",
    "                rendered_img.set_data(state)\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "                \n",
    "        if episode % 10 == 0:\n",
    "            print(\"Saving checkpoint\")\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, save_path)\n",
    "\n",
    "def perform_test():\n",
    "    for episode in range(50):\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = random.choice(action_space)\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            x = preprocess(observation)\n",
    "\n",
    "            if render:\n",
    "                title = 'episode{}'.format(episode)\n",
    "                plt.title(title)\n",
    "\n",
    "                # you can directly render x (which is preprocessed game image)\n",
    "                if rendered_img is None:\n",
    "                    rendered_img = plt.imshow(x)\n",
    "                rendered_img.set_data(x)\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Win 0 Lose 21 Reward -21.0 Loss 0.0006195407477207482\n",
      "Episode 2 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 3 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 4 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 5 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 6 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 7 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 8 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 9 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 10 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Saving checkpoint\n",
      "Episode 11 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 12 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 13 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 14 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 15 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 16 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 17 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 18 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 19 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 20 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Saving checkpoint\n",
      "Episode 21 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 22 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 23 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 24 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 25 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 26 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 27 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 28 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 29 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 30 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Saving checkpoint\n",
      "Episode 31 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 32 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 33 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 34 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 35 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 36 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 37 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 38 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 39 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 40 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Saving checkpoint\n",
      "Episode 41 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 42 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 43 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 44 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 45 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 46 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 47 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 48 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 49 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 50 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Saving checkpoint\n",
      "Episode 51 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 52 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 53 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 54 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 55 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 56 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 57 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 58 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 59 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Episode 60 Win 0 Lose 21 Reward -21.0 Loss 0.0\n",
      "Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "perform_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
